### 1. Closed world
from google.colab import drive
import pickle
import numpy as np
drive.mount('/content/drive')
# 데이터 로드

with open("./drive/MyDrive/data/processed_mon_data_closed_world.pkl", "rb") as f:
    data = pickle.load(f)
X = np.column_stack(list(data['features'].values()))
y = np.array(data['y'])  # 라벨은 그대로 사용
# 데이터 분할

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
# 모델 학습 - Random Forest

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 평가

from sklearn.metrics import classification_report, accuracy_score
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

### 2. Open-world Binary Classification
# 데이터 로드:
with open("./drive/MyDrive/data/processed_binary_data_open_world.pkl", "rb") as f:
    data = pickle.load(f)
X = np.column_stack(list(data['features'].values()))
y = np.array(data['y'])


# 데이터 분할:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)


# 하이퍼파라미터 튜닝

from sklearn.svm import SVC
model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
model.fit(X_train, y_train)

# 모델 평가:
from sklearn.metrics import classification_report, accuracy_score
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

### 3. Open-world Multi-class Classification
# 데이터 로드:
with open("./drive/MyDrive/data/processed_multiclass_data_open_world.pkl", "rb") as f:
    data = pickle.load(f)
X = np.column_stack(list(data['features'].values()))
y = np.array(data['y'])


# 데이터 분할:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)


# 하이퍼파라미터 튜닝

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

#모델 평가:

from sklearn.metrics import classification_report, accuracy_score
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

### 추가: 각 시나리오 공통 추천
# 하이퍼파라미터 튜닝: SVM
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}
grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best Parameters:", grid.best_params_)


# 하이퍼파라미터 튜닝:Random Forest:

param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}
grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best Parameters:", grid.best_params_)


# 교차 검증:
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print("Cross-Validation Scores:", scores)
print("Mean Accuracy:", scores.mean())


# 특성 중요도 분석 (Random Forest):
import matplotlib.pyplot as plt
feature_importances = model.feature_importances_
plt.bar(range(len(feature_importances)), feature_importances)
plt.xlabel("Feature Index")
plt.ylabel("Importance")
plt.title("Feature Importance")
plt.show()

